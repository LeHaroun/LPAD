# -*- coding: utf-8 -*-
"""License_Plate_Detection_OCR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-bDfYGe5oRpOMAdmCu6aJox9cE92BUvD

# License Plate Detection and OCR
This notebook demonstrates a workflow for detecting license plates in images and extracting text from them using YOLO models.
"""

import cv2
import numpy as np
import pytesseract
from PIL import Image

# normal CPU TF
import tensorflow as tf

"""
## Model Loading
Here we define classes for plate detection and OCR and load the pre-trained models.
"""

class PlateDetector:
    def load_model(self, weight_path: str, cfg_path: str):
        self.net = cv2.dnn.readNet(weight_path, cfg_path)
        with open("classes-detection.names", "r") as f:
            self.classes = [line.strip() for line in f.readlines()]
        self.layers_names = self.net.getLayerNames()
        self.output_layers = [self.layers_names[i - 1] for i in self.net.getUnconnectedOutLayers()]

    def load_image(self, img_path):
        img = cv2.imread(img_path)
        if img is None:
            raise FileNotFoundError(f"Unable to load image: {img_path}")
        height, width, channels = img.shape
        return img, height, width, channels

    def detect_plates(self, img):
        blob = cv2.dnn.blobFromImage(img, scalefactor=0.00392, size=(320, 320), mean=(0, 0, 0), swapRB=True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)
        return blob, outputs

    def draw_labels(self, boxes, confidences, class_ids, img):
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.1, 0.1)
        plates = []
        for i in indexes:
            try:
                x, y, w, h = boxes[i]
                label = str(self.classes[class_ids[i]])
                crop_img = self.crop_and_resize(img, x, y, w, h)
                plates.append(crop_img)
                self.draw_box_and_text(img, x, y, w, h, confidences[i])
            except Exception as err:
                print(f"Error processing box {i}: {err}")

        return img, plates

    def crop_and_resize(self, img, x, y, w, h):
        crop_img = img[y:y+h, x:x+w]
        return cv2.resize(crop_img, dsize=(470, 110))

    def draw_box_and_text(self, img, x, y, w, h, confidence):
        font = cv2.FONT_HERSHEY_PLAIN
        color_green = (0, 255, 0)
        cv2.rectangle(img, (x, y), (x + w, y + h), color_green, 8)
        cv2.putText(img, f"{round(confidence, 3) * 100}%", (x + 20, y - 20), font, 12, color_green, 6)

    def get_boxes(self, outputs, width, height, threshold=0.3):
        boxes = []
        confidences = []
        class_ids = []

        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]

                if confidence > threshold:
                    center_x, center_y, w, h = self.extract_box_dimensions(detection, width, height)

                    # Validate box dimensions
                    x, y, w, h = self.validate_box_dimensions(center_x, center_y, w, h, width, height)

                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)

        return boxes, confidences, class_ids

    def extract_box_dimensions(self, detection, width, height):
        center_x = int(detection[0] * width)
        center_y = int(detection[1] * height)
        w = int(detection[2] * width)
        h = int(detection[3] * height)
        return center_x, center_y, w, h

    def validate_box_dimensions(self, center_x, center_y, w, h, width, height):
        x = max(0, min(center_x - w // 2, width - w))
        y = max(0, min(center_y - h // 2, height - h))
        w = min(w, width - x)
        h = min(h, height - y)
        return x, y, w, h


class PlateReader:
    def __init__(self):
        self.font = cv2.FONT_HERSHEY_PLAIN

    def load_model(self, weight_path: str, cfg_path: str):
        self.net = cv2.dnn.readNet(weight_path, cfg_path)
        with open("classes-ocr.names", "r") as f:
            self.classes = [line.strip() for line in f.readlines()]
        self.layers_names = self.net.getLayerNames()

        unconnected_layers = self.net.getUnconnectedOutLayers()
        if unconnected_layers.ndim == 1:  # Scalar indices
            self.output_layers = [self.layers_names[i - 1] for i in unconnected_layers]
        else:  # Assume it's an array of indices
            self.output_layers = [self.layers_names[i[0] - 1] for i in unconnected_layers]

        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))

    def load_image(self, img_path):
        img = cv2.imread(img_path)
        if img is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        height, width, channels = img.shape
        return img, height, width, channels

    def read_plate(self, img):
        blob = cv2.dnn.blobFromImage(img, scalefactor=0.00392, size=(320, 320), mean=(0, 0, 0), swapRB=True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)
        return blob, outputs

    def get_boxes(self, outputs, width, height, threshold=0.3):
        boxes = []
        confidences = []
        class_ids = []
        for output in outputs:
            for detect in output:
                scores = detect[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                if confidence > threshold:
                    center_x = int(detect[0] * width)
                    center_y = int(detect[1] * height)
                    w = int(detect[2] * width)
                    h = int(detect[3] * height)
                    x = int(center_x - w/2)
                    y = int(center_y - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        return boxes, confidences, class_ids

    def draw_labels(self, boxes, confidences, class_ids, img):
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
        characters = []
        for i in indexes:
            box = boxes[i]
            x, y, w, h = box
            self.draw_label(img, x, y, w, h, confidences[i], i)
            label = str(self.classes[class_ids[i]])
            characters.append((label, x))
        characters.sort(key=lambda x: x[1])
        plate = self.convert_to_plate_string(characters)
        return img, plate

    def draw_label(self, img, x, y, w, h, confidence, i):
        color = self.colors[i % len(self.colors)]
        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
        cv2.putText(img, f"{confidence:.2f}%", (x, y - 6), self.font, 1, color, 2)

    def convert_to_plate_string(self, characters):
        plate = ""
        for label, _ in characters:
            plate += self.convert_to_arabic_if_needed(label)
        return plate

    def convert_to_plate_string(self, characters):
        plate = ""
        for label, _ in characters:
            plate += self.convert_to_arabic_if_needed(label)

        # Handle the specific pattern of numbers followed by 'ww'
        plate = self.handle_ww_pattern(plate)

        return plate

    def handle_ww_pattern(self, plate):
        if 'ww' in plate:
            # Extract the number part before 'ww'
            number_part = plate.split('ww')[0]
            # Remove any spaces or decorative elements
            number_part = ''.join(filter(str.isdigit, number_part))
            # Reconstruct the plate with 'ww'
            return number_part + ' ww'
        return plate

    def convert_to_arabic_if_needed(self, label):
        arabic_mappings = {'أ': 'A', 'ب': 'B', 'ج': 'J', 'د': 'D', 'ه': 'H', 'و': 'W', 'ي': 'Y'}
        return arabic_mappings.get(label, label)

    def tesseract_ocr(self, image, lang="eng", psm=7):
        try:
            alphanumeric = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
            options = f"-l {lang} --psm {psm} -c tessedit_char_whitelist={alphanumeric}"
            return pytesseract.image_to_string(image, config=options)
        except Exception as e:
            print(f"OCR Error: {e}")
            return ""


# Load models, check conflicts with opencv !
detector = PlateDetector()
detector.load_model('weights/yolov3-detection_final.weights', 'weights/yolov3-detection.cfg')

reader = PlateReader()
reader.load_model('weights/yolov3-ocr_final.weights', 'weights/yolov3-ocr.cfg')

"""
## Integrated Processing Pipeline
This function integrates image loading, detection, and OCR.
"""

def process_image(image_path):
    # Load image
    img, height, width, channels = detector.load_image(image_path)

    # Detect plates
    blob, outputs = detector.detect_plates(img)
    boxes, confidences, class_ids = detector.get_boxes(outputs, width, height, threshold=0.3)
    plate_img, LpImg = detector.draw_labels(boxes, confidences, class_ids, img)

    # Check if any plates were detected
    if not LpImg:
        return "No plates detected, Try to change Camera angle or Lighting Conditions"

    # Perform OCR on the first detected plate
    ocr_image = LpImg[0]  # assuming the first detected plate
    blob, outputs = reader.read_plate(ocr_image)
    boxes, confidences, class_ids = reader.get_boxes(outputs, width, height, threshold=0.3)
    segmented, plate_text = reader.draw_labels(boxes, confidences, class_ids, ocr_image)

    # Return OCR result
    return plate_text


# Usage usage
print("Normal cases")
print(process_image('images/Example1.jpg'))
print(process_image('images/Example2.jpg'))
print(process_image('images/Example3.jpg'))
print("Edge cases 1")
print(process_image('images/Example4.jpg'))
print(process_image('images/Example5.jpg'))
print("Edge cases 2")
print(process_image('images/Example6.jpg'))
print(process_image('images/Example7.jpg'))
print(process_image('images/Example8.jpg'))



